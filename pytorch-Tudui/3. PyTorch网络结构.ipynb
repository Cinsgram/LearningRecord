{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. PyTorch网络结构**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 神经网络的基本骨架：`nn_model`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PyTorch中，所有的神经网络模块都应该继承自`nn.Module`类，这个类为神经网络提供了一个基本的框架。通过继承`nn.Module`，可以确保自定义网络模块正确地集成了所有必要的功能，并能与PyTorch的其他功能无缝协作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module): # 定义一个名为 NeuralNetwork的类，继承自PyTorch的nn.Module基类\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()  # 调用父类的构造函数来进行初始化\n",
    "    \n",
    "    def forward(self, input):  # 重写forward方法，定义模型的前向传播\n",
    "        output = input + 1\n",
    "        return output\n",
    "    \n",
    "network = NeuralNetwork()\n",
    "x = torch.tensor(1.0)  # 创建一个值为1.0的tensor\n",
    "output = network(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过这个简单的例子，详细解释如何使用`nn.Module`创建一个基本的神经网络：\n",
    "\n",
    "1. **继承 `nn.Module`**\n",
    "\n",
    "首先，我们定义了一个新的类，这个类继承自`nn.Module`。通过继承，该类将获得`nn.Module`提供的所有功能，比如参数管理、梯度传播等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralNetwork, self).__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **使用`super().__init__()`**\n",
    "\n",
    "在类的构造函数中（即`__init__`方法），需要调用`super().__init__()`。这一步非常关键，它确保了父类`nn.Module`的构造函数被正确执行。这样，网络就能正确集成`nn.Module`的所有基础功能。\n",
    "\n",
    "我们可以把`super().__init__()`理解为是在告诉PyTorch：“请确保我的网络有你提供的所有酷炫功能，比如自动管理参数、梯度等。”\n",
    "\n",
    "1. **定义前向传播`forward()`**\n",
    "在PyTorch中，模型如何处理输入数据，是通过`forward()`方法定义的。这是实际执行计算的地方。\n",
    "\n",
    "`forward()`就像是网络的大脑，当我们给它输入数据时，它告诉网络该如何思考和输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward(self, input):\n",
    "#         output = input + 1\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **使用模型**\n",
    "\n",
    "创建模型的实例，并向其传递数据，模型会自动调用`forward()`方法处理数据。\n",
    "\n",
    "创建模型实例后，我们可以像调用函数一样使用它。这样做时，PyTorch背后会智能地调用`forward()`方法，我们不需要显式地去做这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = NeuralNetwork()\n",
    "# x = torch.tensor(1.0)  # 创建一个值为1.0的tensor\n",
    "# output = network(x)    # 自动调用forward方法\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 卷积操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在卷积操作中，一个卷积核在原始图像上逐步滑动，每次移动一格。在每个位置，卷积核与其覆盖的图像区域进行元素间的乘积，然后将这些乘积结果相加得到一个单一的输出值。这一过程在整个图像上重复执行，从而生成一个新的特征图，该特征图是原图经过局部加权和计算后的结果。为了处理图像边缘，通常会在原图周围添加额外的零填充。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"./image/3-1.gif\" width=\"60%\"><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([3, 3]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input = torch.tensor([[1, 2, 0, 3, 1],\n",
    "                      [0, 1, 2, 3, 1],\n",
    "                      [1, 2, 1, 0, 0],\n",
    "                      [5, 2, 3, 1, 1],\n",
    "                      [2, 1, 0, 1, 1]])\n",
    "kernel = torch.tensor([[1, 2, 1],\n",
    "                       [0, 1, 0],\n",
    "                       [2, 1, 0]])\n",
    "input.shape, kernel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. input = torch.reshape(input, (1, 1, 5, 5))\n",
    "​原始形状：input 是 5x5 的二维张量（如一个单通道图像）。\n",
    "​目标形状：(1, 1, 5, 5)，即四维张量：\n",
    "​维度含义：(batch_size, channels, height, width)\n",
    "​具体值：\n",
    "batch_size=1：表示单样本输入。\n",
    "channels=1：表示单通道（如灰度图）。\n",
    "height=5 和 width=5：保留原始图像的尺寸。\n",
    "​用途：适配PyTorch卷积层（如 nn.Conv2d）的输入格式。\n",
    "2. kernel = torch.reshape(kernel, (1, 1, 3, 3))\n",
    "​原始形状：kernel 是 3x3 的二维张量（如一个单通道卷积核）。\n",
    "​目标形状：(1, 1, 3, 3)，即四维张量：\n",
    "​维度含义：(out_channels, in_channels, kernel_height, kernel_width)\n",
    "​具体值：\n",
    "out_channels=1：表示输出通道数（单个卷积核）。\n",
    "in_channels=1：表示输入通道数（与输入图像的通道数一致）。\n",
    "kernel_height=3 和 kernel_width=3：保留原始卷积核的尺寸。\n",
    "​用途：适配PyTorch卷积操作的权重形状要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 2, 0, 3, 1],\n",
      "          [0, 1, 2, 3, 1],\n",
      "          [1, 2, 1, 0, 0],\n",
      "          [5, 2, 3, 1, 1],\n",
      "          [2, 1, 0, 1, 1]]]])\n",
      "tensor([[[[1, 2, 1],\n",
      "          [0, 1, 0],\n",
      "          [2, 1, 0]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 5, 5]), torch.Size([1, 1, 3, 3]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.reshape(input, (1, 1, 5, 5))\n",
    "kernel = torch.reshape(kernel, (1, 1, 3, 3))\n",
    "print(input)\n",
    "print(kernel)\n",
    "input.shape, kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10, 12, 12],\n",
       "          [18, 16, 16],\n",
       "          [13,  9,  3]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv2d(input, kernel, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"./image/3-2.png\" width=\"80%\"><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10, 12],\n",
       "          [13,  3]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 步伐为2\n",
    "F.conv2d(input, kernel, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1,  3,  4, 10,  8],\n",
       "          [ 5, 10, 12, 12,  6],\n",
       "          [ 7, 18, 16, 16,  8],\n",
       "          [11, 13,  9,  3,  4],\n",
       "          [14, 13,  9,  7,  4]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding=1表示二维向量的四边向外拓展一个单位\n",
    "F.conv2d(input, kernel, stride=1, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Stride（步幅）**：卷积核在输入特征图上的采样间隔。设置步幅的主要目的是减少输入的参数数量，从而降低计算量。\n",
    "\n",
    "2. **Padding（填充）**：在输入特征图的每一边均匀添加一定数量的行和列。设置填充的目的是为了使每个输入单元都有机会成为卷积窗口的中心，或者保持输出特征图的尺寸与输入特征图的尺寸相同。\n",
    "\n",
    "对于一个尺寸为 $a \\times a$ 的特征图，通过一个 $b \\times b$ 的卷积核，步幅为 $c$，填充为 $d$，输出特征图的尺寸计算如下：\n",
    "\n",
    "- 若 $d = 0$（无填充），输出特征图的尺寸为 $\\left\\lfloor \\frac{a - b}{c} + 1 \\right\\rfloor$。\n",
    "- 若 $d \\neq 0$（有填充），输出特征图的尺寸为 $\\left\\lfloor \\frac{a + 2d - b}{c} + 1 \\right\\rfloor$。\n",
    "\n",
    "请注意，输出尺寸的计算应使用向下取整函数（$\\lfloor \\cdot \\rfloor$），确保在所有情况下都能得到整数的输出尺寸。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$例1$：一个特征图尺寸为 $4×4$ 的输入，使用 $3×3$ 的卷积核，`stride` = $1$，`padding` = $0$，输出的尺寸 = $\\frac{4-3}{1}+1=2$。\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"./image/3-3.gif\" width=\"25%\"><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$例2$：一个特征图尺寸为 $5×5$ 的输入，使用 $3×3$ 的卷积核，`stride` = $1$，`padding` = $1$，输出的尺寸 = $\\frac{5+2×1-3}{1}+1=5$。\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"./image/3-4.gif\" width=\"40%\"><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$例3$：一个特征图尺寸为 $5×5$ 的输入， 使用 $3×3$ 的卷积核，`stride` = $2$，`padding` = $0$，输出的尺寸 = $\\frac{5-3}{2}+1=2$。\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"./image/3-5.gif\" width=\"30%\"><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$例4$：一个特征图尺寸为 $6×6$ 的输入， 使用 $3×3$ 的卷积核，`stride` = $2$，`padding` = $1$，输出的尺寸 = $\\frac{6+2×1-3}{2}+1=2.5+1=3.5$，向下取整 = $3$（降采样：边长减少 $\\frac{1}{2}$）。\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"./image/3-6.gif\" width=\"50%\"><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多的例子可以在[github](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)上查看。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 卷积层**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **关于卷积类型**：Conv1d、Conv2d和Conv3d分别代表一维、二维和三维卷积，用于处理不同维度的数据，例如时间序列数据（一维）、图像（二维）和视频或体积数据（三维）。\n",
    "\n",
    "2. **关于`kernel_size`**：在训练神经网络的过程中，`kernel_size` 是一个重要的超参数，其定义了卷积核的尺寸。例如，`kernel_size = 3` 通常意味着使用 $3 \\times 3$ 的卷积核。这个参数的选择依赖于想要捕捉的特征的局部性和复杂性。\n",
    "\n",
    "3. **关于输入输出参数的描述**：对于一个具有输入维度 $(N, C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ 的神经网络，其输出维度可以通过以下公式计算：\n",
    "\n",
    "$Input: (N, C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$\n",
    "\n",
    "$Output: (N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}}) \\quad where$\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1 \\right\\rfloor\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{in}} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1 \\right\\rfloor\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Conv2d`是PyTorch中用于进行二维卷积操作的类，广泛应用于处理图像数据。它的参数主要用于定义卷积层的属性和行为。下面是每个参数的详细解释以及常见的设置：\n",
    "\n",
    "1. **in_channels (int)**：代表输入数据的通道数。例如，彩色RGB图像的`in_channels`为3。\n",
    "\n",
    "2. **out_channels (int)**：代表输出数据的通道数，也即卷积核的数量。这个参数决定了网络层可以学习的特征数量。\n",
    "\n",
    "3. **kernel_size (_size_2_t)**：卷积核的尺寸，可以是单个整数或一个`(height, width)`的元组。常见的设置有`3`（意味着`3x3`）、`5`（`5x5`）等。\n",
    "\n",
    "4. **stride (_size_2_t)**：卷积时滑动的步长，可以是单个整数或一个`(vertical_stride, horizontal_stride)`的元组。常见的设置为`1`或`2`，较大的步幅可用于减少输出的空间维度（高度和宽度）。\n",
    "\n",
    "5. **padding (Union[str, _size_2_t])**：输入的边缘填充量，可以是`'same'`（自动调整以保持输出尺寸与输入相同）、`'valid'`（不进行填充）、或一个整数/元组指定每一边的填充像素数。例如，`1`代表每边填充1像素。\n",
    "\n",
    "6. **dilation (_size_2_t)**：卷积核中元素之间的间距，常用于扩大卷积核的感受野。一般默认为`1`，意味着没有膨胀。\n",
    "\n",
    "7. **groups (int)**：控制输入和输出之间的连接（分组卷积）。`groups=1`表示普通的卷积操作，而`groups=in_channels`时，每个输入通道与一个输出通道分组，实现深度可分离卷积。\n",
    "\n",
    "8. **bias (bool)**：是否在卷积层后添加一个可学习的偏置项。通常设置为`True`，除非后续立即使用批归一化层（BatchNorm），这时可能会设置为`False`。\n",
    "\n",
    "9. **padding_mode (str)**：定义填充的类型，`'zeros'`是最常用的，表示用0填充边界。其他选项可能包括`'reflect'`或`'replicate'`，分别对应反射和复制边缘像素的填充方式。\n",
    "\n",
    "10. **device**：指定卷积层参数应存储在哪个设备上，如`'cuda:0'`或`'cpu'`。如果未指定，则使用默认设备。\n",
    "\n",
    "11. **dtype**：指定卷积层参数的数据类型，如`torch.float32`或`torch.float64`。如果未指定，则使用全局默认类型。\n",
    "\n",
    "这些参数共同定义了一个卷积层的结构和行为，可以根据具体的应用需求进行调整。例如，在图像分类任务中，通常会使用小的卷积核（如`3x3`）和步幅为1的标准卷积层，逐渐增加层数和通道数以提取更复杂的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 14:19:45.303409: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-28 14:19:45.486077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743142785.556126    1627 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743142785.578156    1627 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743142785.737171    1627 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743142785.737202    1627 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743142785.737203    1627 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743142785.737204    1627 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-28 14:19:45.756306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10('./data', train=False,\n",
    "                                       transform=torchvision.transforms.ToTensor(),\n",
    "                                       download=False)\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "# 搭建卷积层\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # 彩色图像输入为3层，我们让它的输出为6层，选3×3的卷积\n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "\n",
    "network = NeuralNetwork()\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([16, 3, 32, 32])\n",
      "torch.Size([16, 6, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "# 卷积层处理图片\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    output = network(imgs)\n",
    "    print(imgs.shape)       # 输入为3通道、32×32的64张图片\n",
    "    print(output.shape)     # 输出为6通道、30×30的64张图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard显示\n",
    "step = 0\n",
    "writer = SummaryWriter('./log')\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    output = network(imgs)\n",
    "    # print(imgs.shape)\n",
    "    # torch.Size([64, 3, 32, 32])\n",
    "    # print(output.shape)\n",
    "    # torch.Size([64, 6, 30, 30])\n",
    "    writer.add_images('Convolution_Input', imgs, step)\n",
    "    # 把原来6个通道拉为3个通道，为了保证所有维度总数不变，其余的分量分到第一个维度中\n",
    "    output = torch.reshape(output, (-1, 3, 30, 30))\n",
    "    writer.add_images('Convolution_Output', output, step)\n",
    "    \n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.reshape(output, (-1, 3, 30, 30))`这行代码实际上是在尝试将原始的`output`张量重塑为一个新的形状。这里的`-1`是一个自动计算该维度大小的占位符，它使得PyTorch可以自动计算这个维度应有的大小，以保持元素总数不变。现在我们来详细解释这个过程：\n",
    "\n",
    "**原始形状和目标形状**：\n",
    "\n",
    "- **原始形状**：`[64, 6, 30, 30]` 表示有64个样本，每个样本有6个通道，每个通道的图像大小为30x30。\n",
    "- **目标形状**：`(-1, 3, 30, 30)` 表示想要重塑成某个数量的样本，每个样本有3个通道，每个通道的图像大小为30x30。\n",
    "\n",
    "**计算自动维度`-1`**：\n",
    "\n",
    "在这种情况下，PyTorch将计算`-1`应该代表的维度大小，这个计算基于保持元素总数不变的原则。我们可以这样计算：\n",
    "1. **计算总元素数**：`64 * 6 * 30 * 30 = 345600`。\n",
    "2. **计算每个新样本的元素数**：`3 * 30 * 30 = 2700`。\n",
    "3. **计算新样本的数量**：`345600 / 2700 = 128`。\n",
    "\n",
    "所以，`(-1, 3, 30, 30)` 实际上会被解释为 `(128, 3, 30, 30)`。这意味着原本64个6通道的图像被重组成128个3通道的图像。这种重塑改变了图像数据的结构，每个新的3通道图像包含了原始两个6通道图像的一部分。\n",
    "\n",
    "这种变化可能会导致每个新图像失去原有的意义或上下文，因为原来每个图像的6个通道可能包含特定的信息或特征，简单的重塑可能打乱了这些信息。特别是在图像处理和机器学习任务中，通道之间的关系很重要，因此，这种重塑可能不适合所有应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 最大池化的使用**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **最大池化层通常也被称为下采样层**，因为它通过取区域内的最大值来减少数据的空间尺寸。最大池化层可以保留输入的特征，同时减少数据量。\n",
    "\n",
    "2. **空洞卷积**（`dilation`）通过在标准卷积核的元素之间引入空间间隔来增大其感受野，如下图所示。\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"./image/3-7.gif\" width=\"40%\"><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `ceil_mode`**参数在池化时用于处理边界情况**。当池化窗口超出输入数据的边界时，如果`ceil_mode=True`，则会取覆盖最右下角区域的值，即使这个区域超出了原始输入的大小。\n",
    "\n",
    "4. **池化操作通过减少数据的空间维度来减少计算所需的参数量**。例如，在处理视频数据时，将1080P的视频通过池化降低到720P或360P可以显著减少所需的数据量，从而在相同的网络速度下减少视频播放的卡顿现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 池化操作（例如最大池化）通常在卷积之后应用，用于进一步降低特征图的空间维度。它通过选取输入区域的最大值（或平均值，取决于池化类型）来操作。例如：\n",
    "> \n",
    "> 假设以下是经过卷积后的特征矩阵，我们应用一个 2x2 的最大池化：\n",
    "> \n",
    "> 特征矩阵:\n",
    "> $$\n",
    "> \\begin{bmatrix}\n",
    "> 4 & 4 & 2 \\\\\n",
    "> 4 & 4 & 2 \\\\\n",
    "> 3 & 3 & 1 \\\\\n",
    "> \\end{bmatrix}\n",
    "> $$\n",
    "> 应用2x2最大池化，结果将是：\n",
    "> $$\n",
    "> \\begin{bmatrix}\n",
    "> \\max(4, 4, 4, 4) & \\max(4, 2, 4, 2) \\\\\n",
    "> \\max(4, 4, 3, 3) & \\max(4, 2, 3, 1) \\\\\n",
    "> \\end{bmatrix}\n",
    "> $$\n",
    "> 结果是：\n",
    "> $$\n",
    "> \\begin{bmatrix}\n",
    "> 4 & 4 \\\\\n",
    "> 4 & 4 \\\\\n",
    "> \\end{bmatrix}\n",
    "> $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[1, 2, 0, 3, 1],\n",
       "           [0, 1, 2, 3, 1],\n",
       "           [1, 2, 1, 0, 0],\n",
       "           [5, 2, 3, 1, 1],\n",
       "           [2, 1, 0, 1, 1]]]]),\n",
       " torch.Size([1, 1, 5, 5]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import MaxPool2d\n",
    "\n",
    "input = torch.tensor([[1, 2, 0, 3, 1],\n",
    "                      [0, 1, 2, 3, 1],\n",
    "                      [1, 2, 1, 0, 0],\n",
    "                      [5, 2, 3, 1, 1],\n",
    "                      [2, 1, 0, 1, 1]])\n",
    "input = torch.reshape(input, (-1, 1, 5, 5))\n",
    "input, input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`MaxPool2d`进行最大池化操作，**如果没有显式指定`stride`参数，则默认与`kernel_size`参数相同**。\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"./image/3-8.png\" width=\"80%\"><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ceil_mode=False\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.maxpool = MaxPool2d(kernel_size=3, ceil_mode=False)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.maxpool(input)\n",
    "        return output\n",
    "\n",
    "network = NeuralNetwork()\n",
    "network(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2, 3],\n",
       "          [5, 1]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ceil_mode=True\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.maxpool = MaxPool2d(kernel_size=3, ceil_mode=True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.maxpool(input)\n",
    "        return output\n",
    "\n",
    "network = NeuralNetwork()\n",
    "network(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 池化层处理图片\n",
    "dataset = torchvision.datasets.CIFAR10(\"./data\", train=False,\n",
    "                                       transform=torchvision.transforms.ToTensor(),\n",
    "                                       download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "step = 0\n",
    "writer = SummaryWriter(\"./log\")\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    writer.add_images(\"MaxPool_Input\", imgs, step)\n",
    "    output = network(imgs)\n",
    "    writer.add_images(\"MaxPool_Output\", output, step)\n",
    "    step = step + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5 非线性激活层**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设池化操作后我们得到了以下3x3的特征图：\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "4 & 7 & 6 \\\\\n",
    "8 & 3 & 5 \\\\\n",
    "2 & 9 & 7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "接下来，我们将对这个特征图应用Sigmoid函数，即 $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $，来增加非线性特性。这个激活函数可以帮助网络学习复杂的模式。\n",
    "\n",
    "对每个元素进行Sigmoid变换的计算如下：\n",
    "\n",
    "1. $ \\sigma(4) = \\frac{1}{1 + e^{-4}} \\approx \\frac{1}{1 + 0.018} \\approx 0.982 $\n",
    "2. $ \\sigma(7) = \\frac{1}{1 + e^{-7}} \\approx \\frac{1}{1 + 0.001} \\approx 0.9991 $\n",
    "3. $ \\sigma(6) = \\frac{1}{1 + e^{-6}} \\approx \\frac{1}{1 + 0.0025} \\approx 0.9975 $\n",
    "4. $ \\sigma(8) = \\frac{1}{1 + e^{-8}} \\approx \\frac{1}{1 + 0.0003} \\approx 0.9997 $\n",
    "5. $ \\sigma(3) = \\frac{1}{1 + e^{-3}} \\approx \\frac{1}{1 + 0.05} \\approx 0.9526 $\n",
    "6. $ \\sigma(5) = \\frac{1}{1 + e^{-5}} \\approx \\frac{1}{1 + 0.0067} \\approx 0.9933 $\n",
    "7. $ \\sigma(2) = \\frac{1}{1 + e^{-2}} \\approx \\frac{1}{1 + 0.135} \\approx 0.8808 $\n",
    "8. $ \\sigma(9) = \\frac{1}{1 + e^{-9}} \\approx \\frac{1}{1 + 0.0001} \\approx 0.9999 $\n",
    "9. $ \\sigma(7) = \\frac{1}{1 + e^{-7}} \\approx \\frac{1}{1 + 0.001} \\approx 0.9991 $\n",
    "\n",
    "经过Sigmoid激活函数处理后，得到的3x3特征图为：\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.982 & 0.9991 & 0.9975 \\\\\n",
    "0.9997 & 0.9526 & 0.9933 \\\\\n",
    "0.8808 & 0.9999 & 0.9991\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "这里的每个值代表输入元素经过Sigmoid函数的转换，有助于引入非线性，使得神经网络能够学习到更加复杂的数据表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`inplace`为原地替换，若为True，则变量的值被替换。若为False，则会创建一个新变量，将函数处理后的值赋值给新变量，原始变量的值没有修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "input = torch.tensor([[1, -0.5],\n",
    "                      [-1, 3]])\n",
    "input = torch.reshape(input, (-1, 1, 2, 2))\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6700, 0.9301])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.relu1 = ReLU()\n",
    "        self.sigmoid1 = Sigmoid()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.sigmoid1(input)\n",
    "        return output\n",
    "\n",
    "network = NeuralNetwork()\n",
    "output = network(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.CIFAR10('./data', train=False,\n",
    "                                       transform=torchvision.transforms.ToTensor(),\n",
    "                                       download=False)\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "step = 0\n",
    "writer = SummaryWriter('./log')\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    writer.add_image('NonLinear_Input', imgs, global_step=step, dataformats='NCHW')\n",
    "    output = network(imgs)\n",
    "    writer.add_images('NonLinear_Output', output, step, dataformats='NCHW')\n",
    "    step += 1\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.6 线性层以及其他层**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们经过卷积、池化层等操作之后得到了一个3x3的特征图：\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "2 & 5 & 3 \\\\\n",
    "1 & 4 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "首先，我们需要将这个矩阵展平成一个一维向量以输入到线性层。经过展平操作后的向量 $ \\mathbf{x} $ 为：\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = [2, 5, 3, 1, 4, 6, 7, 8, 9]\n",
    "$$\n",
    "\n",
    "假设线性层设计为输出4个节点，我们有以下参数：\n",
    "\n",
    "- **权重矩阵** $ \\mathbf{W} $，大小为 4x9（因为我们有9个输入特征和4个输出）:\n",
    "\n",
    "$$\n",
    "\\mathbf{W} = \\begin{bmatrix}\n",
    "1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **偏置向量** $ \\mathbf{b} $，长度为 4:\n",
    "\n",
    "$$\n",
    "\\mathbf{b} = [1, 2, 3, 4]\n",
    "$$\n",
    "\n",
    "之后便可计算线性层输出，线性层的输出 $ \\mathbf{y} $ 可通过矩阵乘法和向量加法计算：\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{Wx} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "矩阵乘法 $ \\mathbf{Wx} $ 的计算\n",
    "\n",
    "1. 第一个输出节点:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&(1*2 + 0*5 + 1*3 + 0*1 + 1*4 + 0*6 + 1*7 + 0*8 + 1*9) \\\\\n",
    "&= 2 + 3 + 4 + 7 + 9 \\\\\n",
    "&= 25\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "2. 第二个输出节点:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&(0*2 + 1*5 + 0*3 + 1*1 + 0*4 + 1*6 + 0*7 + 1*8 + 0*9) \\\\\n",
    "&= 5 + 1 + 6 + 8 \\\\\n",
    "&= 20\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "3. 第三个输出节点:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&(1*2 + 1*5 + 1*3 + 1*1 + 1*4 + 1*6 + 1*7 + 1*8 + 1*9) \\\\\n",
    "&= 2 + 5 + 3 + 1 + 4 + 6 + 7 + 8 + 9 \\\\\n",
    "&= 45\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "4. 第四个输出节点:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&(0*2 + 0*5 + 1*3 + 0*1 + 0*4 + 1*6 + 0*7 + 0*8 + 1*9) \\\\\n",
    "&= 3 + 6 + 9 \\\\\n",
    "&= 18\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "添加偏置 $ \\mathbf{b} $：\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = [25 + 1, 20 + 2, 45 + 3, 18 + 4] = [26, 22, 48, 22]\n",
    "$$\n",
    "\n",
    "最终输出向量 $ \\mathbf{y} $ 为:\n",
    "$$\n",
    "\\mathbf{y} = [26, 22, 48, 22]\n",
    "$$\n",
    "\n",
    "这是线性层操作的计算过程，其中权重矩阵和偏置向量对输入向量进行变换，生成了新的特征向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"./image/3-9.png\" width=\"80%\"><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([16, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 49152])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10('./data', train=False,\n",
    "                                       transform=torchvision.transforms.ToTensor(),\n",
    "                                       download=False)\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "# 线性拉平\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    print(imgs.shape)\n",
    "    output = torch.reshape(imgs,(1,1,1,-1))\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([1, 1, 1, 196608])\n",
      "torch.Size([16, 3, 32, 32])\n",
      "torch.Size([1, 1, 1, 49152])\n",
      "torch.Size([1, 1, 1, 49152])\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "network = NeuralNetwork()\n",
    "writer = SummaryWriter(\"./log\")\n",
    "\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    print(imgs.shape)\n",
    "    writer.add_images(\"Linear_Input_1\", imgs, step)\n",
    "    output = torch.reshape(imgs,(1,1,1,-1))     # 方法一：拉平\n",
    "    print(output.shape)\n",
    "    output = network(output)\n",
    "    print(output.shape)\n",
    "    writer.add_images(\"Linear_Output_1\", output, step)\n",
    "    step = step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([196608])\n",
      "torch.Size([196608])\n",
      "torch.Size([16, 3, 32, 32])\n",
      "torch.Size([49152])\n",
      "torch.Size([49152])\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    print(imgs.shape)\n",
    "    writer.add_images(\"Linear_Input_2\", imgs, step)\n",
    "    output = torch.flatten(imgs)        # 方法二：拉平。展开为一维\n",
    "    print(output.shape)\n",
    "    output = network(output)\n",
    "    print(output.shape)\n",
    "    step = step + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.7 搭建小实战和Sequential的使用**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 将网络结构放在 `Sequential` 容器中的主要好处是代码编写更加简洁和直观。这种方式使得层次结构清晰，逐层堆叠，便于理解和维护。\n",
    "\n",
    "2. 可以通过以下公式来计算神经网络每层的参数量，这有助于精确掌握模型的规模和复杂度。通过这种方法，我们能够更有效地设计和优化网络架构。下图为CIFAR 10 Model结构。\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"./image/3-10.png\" width=\"90%\"><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们详细地分析CIFAR 10 Model这个卷积神经网络（CNN）的每一层，特别是关于通道数、卷积核参数等具体细节：\n",
    "\n",
    "**网络的输入**：有3个通道（R, G, B）、大小为32x32的图像。\n",
    "\n",
    "1. **第一层卷积 (Conv1)**\n",
    "- **输入**: 3x32x32的图像（3代表RGB通道，32x32是图像尺寸）。\n",
    "- **卷积核大小**: 5x5（提取5x5像素区域的特征）。\n",
    "- **卷积核数量**: 32（输出32个不同的特征图，每个特征图由不同的卷积核生成）。\n",
    "- **Padding**: 2（在图像的边缘添加两层像素，使用0填充，以确保输出图像大小与输入图像大小相同）。\n",
    "- **Stride**: 默认为1（卷积核移动步长为1）。\n",
    "- **输出**: 32x32x32的特征图（因为使用了padding，所以输出尺寸与输入尺寸相同，但通道数为32）。\n",
    "\n",
    "2. **第一层池化 (MaxPool1)**\n",
    "- **输入**: 32x32x32的特征图。\n",
    "- **池化窗口**: 2x2（减少特征图的尺寸，提取最显著的特征）。\n",
    "- **Stride**: 2（池化窗口每次移动2个像素，这减少了特征图的尺寸）。\n",
    "- **输出**: 32x16x16的特征图（尺寸减半，深度保持不变）。\n",
    "\n",
    "3. **第二层卷积 (Conv2)——与第一层卷积类似**\n",
    "- **输入**: 32x16x16的特征图。\n",
    "- **卷积核大小**: 5x5。\n",
    "- **卷积核数量**: 32。\n",
    "- **Padding**: 2。\n",
    "- **Stride**: 默认为1。\n",
    "- **输出**: 32x16x16的特征图（因为使用了padding，所以输出尺寸与输入尺寸相同，但通道数为32）。\n",
    "\n",
    "4. **第二层池化 (MaxPool2)——与第一层池化类似**\n",
    "- **输入**: 32x16x16的特征图。\n",
    "- **池化窗口**: 2x2。\n",
    "- **Stride**: 2。\n",
    "- **输出**: 32x8x8的特征图（尺寸减半，深度保持不变）。\n",
    "\n",
    "5. **第三层卷积 (Conv3)**\n",
    "- **输入**: 32x8x8的特征图。\n",
    "- **卷积核大小**: 5x5。\n",
    "- **卷积核数量**: 64（这里增加了卷积核数量以捕捉更复杂的特征）。\n",
    "- **Padding**: 2。\n",
    "- **Stride**: 默认为1。\n",
    "- **输出**: 64x8x8的特征图（输出的深度增加，尺寸保持不变）。\n",
    "\n",
    "6. **第三层池化 (MaxPool3)**\n",
    "- **输入**: 64x8x8的特征图。\n",
    "- **池化窗口**: 2x2。\n",
    "- **Stride**: 2。\n",
    "- **输出**: 64x4x4的特征图（尺寸减半，深度保持不变）。\n",
    "\n",
    "7. **Flatten层**\n",
    "- **输入**: 64x4x4的特征图。\n",
    "- **操作**: 将所有特征图展平成一个一维向量，以便可以输入到全连接层。\n",
    "- **输出**: 1024元素的向量（64×4×4=1024）。\n",
    "\n",
    "8. **全连接层 (Linear1)**\n",
    "- **输入**: 1024元素的向量。\n",
    "- **神经元数量**: 64。\n",
    "- **输出**: 64元素的向量。\n",
    "\n",
    "9. **输出层 (Linear2)**\n",
    "- **输入**: 64元素的向量。\n",
    "- **神经元数量**: 10（通常对应分类任务中的类别数）。\n",
    "- **输出**: 10元素的向量，每个元素代表一个类别的得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear1): Linear(in_features=1024, out_features=64, bias=True)\n",
       "  (linear2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # 3表示输入的通道数，32表示输出的通道数\n",
    "        # 5表示卷积核的大小，padding=2表示向四周填充2个像素的0\n",
    "        self.conv1 = Conv2d(3, 32, 5, padding=2)\n",
    "        # 2表示池化窗口的大小\n",
    "        self.maxpool1 = MaxPool2d(2)\n",
    "        self.conv2 = Conv2d(32, 32, 5, padding=2)\n",
    "        self.maxpool2 = MaxPool2d(2)\n",
    "        self.conv3 = Conv2d(32, 64, 5, padding=2)\n",
    "        self.maxpool3 = MaxPool2d(2)\n",
    "        # 将多维输出展平成一维\n",
    "        self.flatten = Flatten()\n",
    "        # 1024表示输入特征的数量，即输入是含有1024个元素的一维向量\n",
    "        # 64表示输出特征的数量，即输出是含有64个元素的一维向量\n",
    "        self.linear1 = Linear(1024, 64)\n",
    "        # 64表示输入特征的数量，10表示输出特征的数量\n",
    "        self.linear2 = Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "network = NeuralNetwork()\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查网络的正确性\n",
    "input = torch.ones((64, 3, 32, 32))\n",
    "output = network(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (model1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=1024, out_features=64, bias=True)\n",
       "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用Sequential容器构建网络\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()        \n",
    "        self.model1 = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "network = NeuralNetwork()\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查网络的正确性\n",
    "input = torch.ones((64, 3, 32, 32))\n",
    "output = network(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard显示网络\n",
    "writer = SummaryWriter(\"./log\")\n",
    "writer.add_graph(network, input)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
